{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDQ2suH4p1sv"
      },
      "source": [
        "# Pip Installs and Connect the Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbYPmIykew0h",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pandas\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "device='cuda'\n",
        "drive.mount('/content/drive')\n",
        "drive_path = 'wav2vec2_final'\n",
        "final = os.path.join('drive', 'My Drive', drive_path)\n",
        "print(os.listdir(final))\n",
        "sys.path.append(final)\n",
        "\n",
        "file_path = final + '/dataaudio'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glAZbSHgpqmt"
      },
      "source": [
        "# Forming the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke9cQTHNwWu_"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import librosa\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "paths = []\n",
        "arr = []\n",
        "text = []\n",
        "\n",
        "file_path = final + '/dataaudio'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6OjhddbwZpO"
      },
      "outputs": [],
      "source": [
        "for i in range(40):\n",
        "  paths.append(file_path + f'/{i+1}.wav')\n",
        "\n",
        "file = file_path + '/text.txt'\n",
        "with open(file) as f:\n",
        "    transcription = f.readlines()\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPa_0jB5e5zW"
      },
      "outputs": [],
      "source": [
        "for i in range(40):\n",
        "  transcription[i] = transcription[i].strip('\\n')\n",
        "   #text.append(transcription[i])\n",
        "  temp,_ = librosa.load(paths[i])\n",
        "  arr.append(temp)\n",
        "\n",
        "dataset = {'array': arr, \n",
        "           'sampling_rate': 16000, \n",
        "           'path': paths, \n",
        "           'transcription': transcription\n",
        "           }\n",
        "\n",
        "df = pd.DataFrame(dataset)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "dataset = dataset.train_test_split(test_size = 0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R81o8e7epmbX"
      },
      "source": [
        "# Construct the pre_trained model processor\n",
        "*Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor and a Wav2Vec2 CTC tokenizer into a single processor.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dH2BOfprfByz"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ioh4fLIpkCY"
      },
      "source": [
        "# Forming Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcI4Jvjnsn3p"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "def prepare_dataset(batch):\n",
        "    batch[\"input_values\"] = processor(batch[\"array\"], sampling_rate=batch[\"sampling_rate\"]).input_values[0]\n",
        "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
        "\n",
        "    with processor.as_target_processor():\n",
        "        batch[\"labels\"] = processor(batch[\"transcription\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(prepare_dataset, num_proc=4)\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "        \n",
        "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB-EPNYqpdze"
      },
      "source": [
        "# Constructing and Training the model\n",
        "*Wav2Vec2ForCTC is used to instantiate a Wav2Vec2 model according to the specified arguments, defining the model architecture. Since the base model is pre-trained on 16 kHz audio, we must make sure our audio sample is also resampled to a 16 kHz sampling rate.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFM5UY82vSVX"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2ForCTC, TrainingArguments, Trainer\n",
        "\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    \"facebook/wav2vec2-base-960h\",\n",
        "    ctc_loss_reduction=\"mean\",\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    group_by_length=True,\n",
        "    per_gpu_train_batch_size=8,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.005,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1vRWfpDpbUT"
      },
      "source": [
        "# INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLH1fEooZgG9"
      },
      "outputs": [],
      "source": [
        "audio_input = dataset['test']['array'][5]\n",
        "input_values = processor(audio_input, sampling_rate = 16000, return_tensors=\"pt\",device='cuda',padding=True).input_values\n",
        "\n",
        "logits = model(input_values.cuda()).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "transcription = processor.decode(predicted_ids[0])\n",
        "print(\"NEW_DECODED: \", transcription)\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "audioText_new = dataset['test']['transcription'][5]\n",
        "\n",
        "\n",
        "from difflib import SequenceMatcher\n",
        "m_n = SequenceMatcher(None, audioText_new.upper(), transcription)\n",
        "\n",
        "print(\"NEW_ACTUAL: \", audioText_new)\n",
        "print(\"\") \n",
        "print (\"NEW_ACCURACY = \", m_n.ratio()*100)\n",
        "print(\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV0Rk8ynZwJU"
      },
      "source": [
        "# Old Data Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from transformers import Wav2Vec2ForCTC\n",
        "\n",
        "processor_old = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model_old = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "audio_input = dataset['test']['array'][5]\n",
        "input_values = processor_old(audio_input, sampling_rate = 16000, return_tensors=\"pt\",padding=True).input_values\n",
        "\n",
        "logits = model_old(input_values).logits\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "\n",
        "transcription = processor_old.decode(predicted_ids[0])\n",
        "\n",
        "audioText_old = dataset['test']['transcription'][5]\n",
        "\n",
        "\n",
        "from difflib import SequenceMatcher\n",
        "m_o = SequenceMatcher(None, audioText_old.upper(), transcription)\n",
        "\n",
        "print(\"OLD_DECODED: \", transcription)\n",
        "print(\"\")\n",
        "print(\"OLD_ACTUAL: \", audioText_old)\n",
        "print(\"\")\n",
        "print (\"OLD_ACCURACY = \", m_o.ratio()*100)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TOVzMqrmNP4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/wav2vec2_final/dataaudio/speak.csv\")\n",
        "df.loc[df['Sentence'] == audioText_new]"
      ],
      "metadata": {
        "id": "rkNIj_osWI8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB_XDM-tzggj"
      },
      "source": [
        "# Graphical Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHmmptKkdOYa"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "138a97IDcM6R"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import gutenberg\n",
        "import matplotlib.pyplot as plt\n",
        "file = open(\"/content/drive/MyDrive/wav2vec2_final/dataaudio/text.txt\")\n",
        "sample = file.read()\n",
        "m=len(sample)\n",
        "token = word_tokenize(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWaXGal37yQt"
      },
      "outputs": [],
      "source": [
        "print(token)\n",
        "t=len(token)\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkJKJTW0b_qh"
      },
      "outputs": [],
      "source": [
        "token = word_tokenize(sample)\n",
        "wlist = []\n",
        "\n",
        "for i in range(len(token)):\n",
        "    wlist.append(token[i])\n",
        "\n",
        "wordfreq = [wlist.count(w) for w in wlist]\n",
        "print(\"Pairs\\n\" + str(zip(token, wordfreq)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVfKEMas8RWU"
      },
      "outputs": [],
      "source": [
        "print(wlist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hQ-leY5edmS"
      },
      "outputs": [],
      "source": [
        "from numpy.core.fromnumeric import argmax\n",
        "print(token)\n",
        "print(wordfreq)\n",
        "len(wordfreq)\n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBIsV4DGAFuR"
      },
      "outputs": [],
      "source": [
        "wordfreq_ind=list(set(zip(token, wordfreq)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70vet0N20LYY"
      },
      "outputs": [],
      "source": [
        "wordfreq_ind.sort(key = lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlUXUvlRywRU"
      },
      "outputs": [],
      "source": [
        "print((wordfreq_ind))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfRFmTigbyTZ"
      },
      "outputs": [],
      "source": [
        "plt.bar(token, wordfreq)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}